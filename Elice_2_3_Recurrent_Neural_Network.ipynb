{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPz6Xvdaw2VAV8k5EgtTgVM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyesungKomet/rokaf_ai/blob/main/Elice_2_3_Recurrent_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN(Recurrent Neural Network\n",
        "\n",
        "## Sequential Data\n",
        "* Sequential data(순차 데이터)를 처리하기 위한 모델 ex) 시계열 데이터\n",
        "  - 데이터 내 객체 간 순서가 중요!\n",
        "  - 날씨, 강수량, 주식 등 -> 예측 : 강수량 예측(Precipitation Forecasting) by Google(MetNet)\n",
        "  - 자연어 데이터 -> 음성인식, 번역기, 챗봇\n",
        "  - 오디오 파일 -> 음악 장르 분석\n",
        "\n",
        "## FC Layer\n",
        "RNN에서 FC Layer(Fully-Connected Layer)가 적합하지 않은 이유\n",
        "* FC Layer은 입력 노드와 출력 노드의 개수가 정해짐 \n",
        "but 순차데이터는 그걸 이루는 객체 수가 바뀔 수 있음\n",
        "  * ex) I am hungry 에서 I am so hungry로 바꾸면 FC Layer에서는 입력이 3개로 정해졌으니 so에서 끊어버려 의미를 완전히 상실해버린다\n",
        "\n",
        "* FC Layer은 순서 고려가 불가능 but 순차데이터는 순서가 중요!\n",
        "\n",
        "## Embedding  \n",
        ": 각 단어들을 숫자로 된 벡터로 변환\n",
        "* 단순한 Encoding\n",
        "\n",
        "![image](https://postfiles.pstatic.net/MjAxODA4MDlfMjI5/MDAxNTMzODIzMTQwNDky.igqtWuHq1_XFexz9jHtUoYhuKBkfdaXzrTMbXXmizmog.k2eHyady3gYgZKwWF2ofZ4IKRZ8YNwQzpdHZhwVjo-cg.PNG.timtaeil/image.png?type=w773)\n",
        "* One-hot Encoding\n",
        "\n",
        "![image.jpg](https://postfiles.pstatic.net/MjAxODA4MDlfMTU2/MDAxNTMzODIzNTc0NTc5.RNpGTrnWJuXen35rQ0YlignF0wdkVjMkrOxsS4gh8IAg.Nb4Ay_AMNnhSlHz0rOiuZhVB6nSv7qqcu8F7t8Ah_BQg.PNG.timtaeil/image.png?type=w773)\n",
        "\n",
        "원핫 인코딩의 문제점  \n",
        "* 공간낭비  \n",
        "  단어가 많아질수록 0으로 채워지는 공간만 많아짐(희소벡터)\n",
        "* 단어의 유사도\n",
        "  원한 인코딩으로는 단어의 유사도를 표현하지 못함\n",
        "  단어 간 유사도는 벡터 간의 거리로 나타난다  \n",
        "  ex) 남자, 여자와 왕, 여왕 간의 벡터의 거리가 비슷함  \n",
        "  but 원핫 인코딩을 하면 모든 벡터간 거리가 같아서 이를 못나타낸다  \n",
        "\n",
        "* Word2Vec: 주어진 단어를 벡터로 변환하는 모델\n",
        "  - CBOW Model\n",
        "  - Skip-Gram Model\n",
        "\n",
        "\n",
        "  ### CBOW Model\n",
        "  : 주변 단어로부터 중심 단어를 추측하는 신경망  \n",
        "  ![](https://lilianweng.github.io/posts/2017-10-15-word-embedding/word2vec-cbow.png)\n",
        "  여기 자세히 나와있다...  \n",
        "  https://lilianweng.github.io/posts/2017-10-15-word-embedding/\n",
        "\n",
        "  ![](https://1.bp.blogspot.com/-gjnDTyhZOmQ/XPDtkX8qtFI/AAAAAAAABtU/lxYFusQZd4UwT9E3aM_mgVj8ldFbYdcawCLcBGAs/s400/image001.png)\n",
        "\n",
        "  윈도우 크기: 주변 단어 몇 개를 볼 지\n",
        "  ex) 윈도우가 1: 주변 단어 2개, 윈도우가 2: 주변 단어 4개\n",
        "\n",
        "* 단어 별 ID를 순서대로 부여하고 다 ID로 임베딩해준 다음 원핫으로 표현하여 모델에 넣는다!\n",
        "\n",
        "* [i like natural language processing] -> Word ID: [0 1 2 3 4]  \n",
        "![](https://1.bp.blogspot.com/-sgsYUYw1AKw/XP9WuOoJD2I/AAAAAAAABuE/KJemBqYMHhErtN7noNMVLIy6nQc5K5FHgCLcBGAs/s1600/Capture.PNG)\n",
        "\n",
        "* ![](https://1.bp.blogspot.com/-NxS7TEIHumY/XP9XBdZzQEI/AAAAAAAABuM/3dAf7K39zMw0g5rCfvaYZvCwwyEfCAH8gCLcBGAs/s1600/Capture.PNG)\n",
        "\n",
        "\n",
        "  ### Skip-Gram Model\n",
        "  : 중심 단어로부터 주변 단어 추측하는 신경망\n",
        "  * 학습 시간은 CBOW가 빠르지만 말뭉치가 커질수록 Skip-Gram이 더 뛰어난 성능을 보인다\n",
        "\n",
        "  CBOW의 반대를 생각하면 될듯... 수치적인거나 모델 구조는 따로 더 공부해야될 듯 하다...\n",
        "\n",
        "단어의 문맥적 의미를 수치적으로 보전하기에 One-hot 인코딩의 단점 극복하여 단어 간 유사성을 나타낼 수 있다!!\n",
        "\n",
        "## Hidden State\n",
        ": Internal state that is updated as a sequence is processed\n",
        "\n",
        "순환구조 구현의 핵심 요소!!\n",
        "\n",
        "## Vanila RNN\n",
        "![](https://jsideas.net/assets/materials/20190216/vanilla_rnn.png)\n",
        "\n",
        "세 개의 FC Layers\n",
        "* $$W_{hh}: hiden state(h_{t-1})을 변환$$\n",
        "* $$W_{xh}: 입력값(x_t)을 변환$$\n",
        "* $$W_{yh}: 출력값(y_t)을 변환$$\n",
        "* $$y_t = W_{hy} h_t$$\n",
        "\n",
        "### Hidden State\n",
        "* 입력값에 대한 새로운 hidden state에 대한 계산이 이루어짐\n",
        "* 이전 시점에 생성된 hidden state를 다음 시점에 이용!\n",
        "* 입력값들의 상관관계 & 경향성 정보 압축 저장\n",
        "* 모델의 내부적인 값 -> \"Memory\"\n",
        "\n",
        "* Parameter Sharing\n",
        "  * Hidden State와 출력값 계산을 위한 모든 FC Layer를 모든 시점의 입력값이 재사용\n",
        "  * 세 FC Layer가 파라미터의 전부!!\n",
        "\n",
        "![](https://blog.acolyer.org/wp-content/uploads/2019/02/rnnvis-fig-2.jpeg?w=480)\n",
        "\n",
        "## Loss\n",
        "* BPTT(Back Propagation Through Time) - 역전파가 시간 흐름에 따라 작동\n",
        "* 각 시점의 출력값과 실제 값 비교하여 Loss 구함\n",
        "\n",
        "\n",
        "## Vanila RNN의 문제점\n",
        "* Gradient Vanishing(기울기 소실)\n",
        "  입력값의 길이가 매우 길어질 경우 - 초기 입력값과 끝의 출력값 사이의 기울기가 매우 작아짐\n",
        "  -> Long Term Dependancy(장기 의존성) 다루기 힘들어짐\n",
        "\n",
        "* HOW? --> LSTM man~\n",
        "\n",
        "## Vanila RNN 종류\n",
        "* many-to-one(다대일 구조)  \n",
        "  한 시점의 출력값만 사용\n",
        "* many-to-many(다대다 구조)  \n",
        "  여러 시점의 입출력값 사용\n",
        "* Encoder-Decoder  \n",
        "  입력값들을 받아 특정 hidden state로 인코딩 후 이를 통해 새로운 출력값 만드는 구조\n",
        "  ![](https://miro.medium.com/max/1400/1*1JcHGUU7rFgtXC_mydUA_Q.jpeg)\n",
        "\n"
      ],
      "metadata": {
        "id": "kA10YpIDAr2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One-hot Encoding 구현"
      ],
      "metadata": {
        "id": "ybU1Azp1Kzze"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6g9p1V5-_VVr"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_token(texts):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "  return tokenizer"
      ],
      "metadata": {
        "id": "yZ7UR-FLLB0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text2seq(text, tokenizer):\n",
        "  return tokenizer.texts_to_sequences([text])[0]\n",
        "\n",
        "def seq2onehot(seq,num_word):\n",
        "  return to_categorical(seq,num_classes=num_word+1)"
      ],
      "metadata": {
        "id": "Rmphz8YHLRAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"stand on the shoulders of giants\"\n",
        "text2 = \"I can stand on mountains\""
      ],
      "metadata": {
        "id": "saCtRy2LLmG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = set_token([text1, text2])"
      ],
      "metadata": {
        "id": "UfyvI36RLrrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"단어 수: \", len(tokenizer.word_index))\n",
        "print(\"단어 인덱스: \", tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWGdrYHALvef",
        "outputId": "4b355285-a142-4f51-b90d-9093e8b796dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 수:  9\n",
            "단어 인덱스:  {'stand': 1, 'on': 2, 'the': 3, 'shoulders': 4, 'of': 5, 'giants': 6, 'i': 7, 'can': 8, 'mountains': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq = text2seq(text2, tokenizer)\n",
        "print(seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B734g4HlL2j2",
        "outputId": "b499ec09-db67-42ba-93a4-eefeeed9ee0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7, 8, 1, 2, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "onehot1 = seq2onehot(seq, len(tokenizer.word_index))\n",
        "print(onehot1)\n",
        "print(\"한 단어를 표현하는 길이: \", len(onehot1[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBX_sqZsMAiV",
        "outputId": "96e24d31-6863-4ebf-d624-a1d363a1e176"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "한 단어를 표현하는 길이:  10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vanila RNN Model 구현"
      ],
      "metadata": {
        "id": "NWljJg53Sy6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Sequential\n",
        "\n",
        "# 첫번째 모델\n",
        "def build_model1():\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(layers.Embedding(10, 5)) #input dim(전체 단어 개수), output dim(벡터 길이)\n",
        "    model.add(layers.SimpleRNN(3)) # hidden state 크기\n",
        "    \n",
        "    return model\n",
        "\n",
        "# 두번째 모델\n",
        "def build_model2():\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(layers.Embedding(256,100))\n",
        "    model.add(layers.SimpleRNN(20))\n",
        "    model.add(layers.Dense(10, activation=\"softmax\"))\n",
        "    \n",
        "    return model\n",
        "    \n",
        "def main():\n",
        "    model1 = build_model1()\n",
        "    print(\"=\" * 20, \"첫번째 모델\", \"=\" * 20)\n",
        "    model1.summary()\n",
        "    \n",
        "    print()\n",
        "    \n",
        "    model2 = build_model2()\n",
        "    print(\"=\" * 20, \"두번째 모델\", \"=\" * 20)\n",
        "    model2.summary()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "ja9q93JHMKPY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e6dd54f-e383-4d7c-d1b5-aa810daccf4a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================== 첫번째 모델 ====================\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 5)           50        \n",
            "                                                                 \n",
            " simple_rnn (SimpleRNN)      (None, 3)                 27        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 77\n",
            "Trainable params: 77\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "==================== 두번째 모델 ====================\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, None, 100)         25600     \n",
            "                                                                 \n",
            " simple_rnn_1 (SimpleRNN)    (None, 20)                2420      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                210       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 28,230\n",
            "Trainable params: 28,230\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vanila RNN으로 IMDb data 학습\n",
        "![](https://static.amazon.jobs/teams/53/thumbnails/IMDb_Jobs_Header_Mobile.jpg?1501027253)\n",
        "\n",
        "IMDb(Internet Movie Database): 영화 정보를 데이터베이스화해서 제공함\n",
        "\n",
        "* 데이터셋: 사용자의 영화 리뷰를 긍정/부정으로 분류(2클래스)\n",
        "\n",
        "* -> Sentimental Analysis(감성 분석)에 많이 쓰임!\n"
      ],
      "metadata": {
        "id": "3gCpBECAcM0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def load_data(num_words, max_len):\n",
        "    # imdb 데이터셋. 데이터셋에서 단어는 num_words 개를 가져옴\n",
        "    (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_words)\n",
        "\n",
        "    # 단어 개수가 다른 문장들을 Padding을 추가하여\n",
        "    # 단어가 가장 많은 문장의 단어 개수로 통일.\n",
        "    X_train = pad_sequences(X_train, maxlen=max_len) # 단어 개수 통일\n",
        "    X_test = pad_sequences(X_test, maxlen=max_len)\n",
        "    \n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def build_rnn_model(num_words, embedding_len):\n",
        "    model = Sequential()\n",
        "    \n",
        "    # 모델\n",
        "    model.add(layers.Embedding(num_words, embedding_len))\n",
        "    model.add(layers.SimpleRNN(16))\n",
        "    model.add(layers.Dense(1,activation=\"sigmoid\")) #긍정일 확률 하나만 뽑는다\n",
        "    \n",
        "    return model\n",
        "\n",
        "def main(model=None, epochs=5):\n",
        "    # IMDb 데이터셋에서 가져올 단어의 개수\n",
        "    num_words = 6000\n",
        "    \n",
        "    # 각 문장이 가질 수 있는 최대 단어 개수\n",
        "    max_len = 130\n",
        "    \n",
        "    # 임베딩 된 벡터의 길이\n",
        "    embedding_len = 100\n",
        "    \n",
        "    # IMDb 데이터셋 불러오기\n",
        "    X_train, X_test, y_train, y_test = load_data(num_words, max_len)\n",
        "    \n",
        "    if model is None:\n",
        "        model = build_rnn_model(num_words, embedding_len)\n",
        "    \n",
        "    # optimizer와 loss 함수\n",
        "    optimizer = Adam(learning_rate=0.001)\n",
        "    model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]) #값 하나가 확률로 해석되는 경우 BCE를 써야 함!\n",
        "    \n",
        "    # 모델 학습을 위한 hyperparameter\n",
        "    hist = model.fit(X_train, y_train, epochs=epochs, batch_size=100, validation_split=0.2, shuffle=True,verbose=2)\n",
        "    \n",
        "    # 모델을 테스트 데이터셋으로 테스트\n",
        "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print()\n",
        "    print(\"테스트 Loss: {:.5f}, 테스트 정확도: {:.3f}%\".format(test_loss, test_acc * 100))\n",
        "    \n",
        "    return optimizer, hist\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f50iDdlpS3NS",
        "outputId": "2232b949-f373-48cd-87ce-a9863761e1bf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "200/200 - 11s - loss: 0.5675 - accuracy: 0.6916 - val_loss: 0.4436 - val_accuracy: 0.8070 - 11s/epoch - 54ms/step\n",
            "Epoch 2/5\n",
            "200/200 - 10s - loss: 0.3683 - accuracy: 0.8429 - val_loss: 0.4404 - val_accuracy: 0.7996 - 10s/epoch - 51ms/step\n",
            "Epoch 3/5\n",
            "200/200 - 9s - loss: 0.2495 - accuracy: 0.9049 - val_loss: 0.4047 - val_accuracy: 0.8308 - 9s/epoch - 46ms/step\n",
            "Epoch 4/5\n",
            "200/200 - 9s - loss: 0.1550 - accuracy: 0.9484 - val_loss: 0.4397 - val_accuracy: 0.8332 - 9s/epoch - 45ms/step\n",
            "Epoch 5/5\n",
            "200/200 - 9s - loss: 0.0832 - accuracy: 0.9772 - val_loss: 0.5026 - val_accuracy: 0.8216 - 9s/epoch - 45ms/step\n",
            "\n",
            "테스트 Loss: 0.50740, 테스트 정확도: 81.912%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 심층 Vanila RNN"
      ],
      "metadata": {
        "id": "AbQ6A12k1hYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "\n",
        "def load_data(num_data, window_size):\n",
        "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, num_data, 1)\n",
        "     # sin 함수로 만든 간단한 시계열 데이터\n",
        "    time = np.linspace(0, 1, window_size + 1)\n",
        "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))\n",
        "    series += 0.1 * np.sin((time - offsets2) * (freq2 * 10 + 10))\n",
        "    series += 0.1 * (np.random.rand(num_data, window_size + 1) - 0.5)\n",
        "    \n",
        "    num_train = int(num_data * 0.8)\n",
        "    X_train, y_train = series[:num_train, :window_size], series[:num_train, -1]\n",
        "    X_test, y_test = series[num_train:, :window_size], series[num_train:, -1]\n",
        "    \n",
        "    X_train = X_train[:, :, np.newaxis]\n",
        "    X_test = X_test[:, :, np.newaxis]\n",
        "    \n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def build_rnn_model(window_size):\n",
        "    model = Sequential()\n",
        "\n",
        "    # SimpleRNN 기반 모델\n",
        "    model.add(layers.SimpleRNN(20, input_shape=(window_size,1)))\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    return model\n",
        "\n",
        "def build_deep_rnn_model(window_size):\n",
        "    model = Sequential()\n",
        "\n",
        "    # 여러개의 SimpleRNN을 가지는 모델\n",
        "    model.add(layers.SimpleRNN(20, return_sequences=True, input_shape=(window_size,1))) \n",
        "    #return_sequences: simplernn은 default: 다대일(마지막 출력만)\n",
        "    # 여기서는 여러 층에서의 출력값을 다 입력값으로 쓰도록 설정함\n",
        "    model.add(layers.SimpleRNN(20))\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    return model\n",
        "\n",
        "def run_model(model, X_train, X_test, y_train, y_test, epochs=20, name=None):\n",
        "    # optimizer와 loss 함수\n",
        "    optimizer = Adam(learning_rate=0.001)\n",
        "    model.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "    #  hyperparameter\n",
        "    hist = model.fit(X_train, y_train, epochs=epochs, batch_size=256,shuffle=True,verbose=2)\n",
        "    \n",
        "    # 테스트 데이터셋으로 모델을 테스트\n",
        "    test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(\"[{}] 테스트 MSE: {:.5f}\".format(name, test_loss))\n",
        "    print()\n",
        "\n",
        "    return optimizer, hist\n",
        "    \n",
        "def main():\n",
        "    tf.random.set_seed(2022)\n",
        "    np.random.seed(2022)\n",
        "\n",
        "    window_size = 50\n",
        "    X_train, X_test, y_train, y_test = load_data(10000, window_size)\n",
        "\n",
        "    rnn_model = build_rnn_model(window_size)\n",
        "    run_model(rnn_model, X_train, X_test, y_train, y_test, name=\"RNN\")\n",
        "\n",
        "    deep_rnn_model = build_deep_rnn_model(window_size)\n",
        "    run_model(deep_rnn_model, X_train, X_test, y_train, y_test, name=\"Deep RNN\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YVBVEm_lkHn",
        "outputId": "98f31e8b-5b48-48c2-e39c-92e18c85eca0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "32/32 - 2s - loss: 0.1129 - 2s/epoch - 53ms/step\n",
            "Epoch 2/20\n",
            "32/32 - 0s - loss: 0.0186 - 364ms/epoch - 11ms/step\n",
            "Epoch 3/20\n",
            "32/32 - 0s - loss: 0.0065 - 360ms/epoch - 11ms/step\n",
            "Epoch 4/20\n",
            "32/32 - 0s - loss: 0.0032 - 332ms/epoch - 10ms/step\n",
            "Epoch 5/20\n",
            "32/32 - 0s - loss: 0.0027 - 339ms/epoch - 11ms/step\n",
            "Epoch 6/20\n",
            "32/32 - 0s - loss: 0.0026 - 357ms/epoch - 11ms/step\n",
            "Epoch 7/20\n",
            "32/32 - 0s - loss: 0.0025 - 340ms/epoch - 11ms/step\n",
            "Epoch 8/20\n",
            "32/32 - 0s - loss: 0.0024 - 333ms/epoch - 10ms/step\n",
            "Epoch 9/20\n",
            "32/32 - 0s - loss: 0.0023 - 342ms/epoch - 11ms/step\n",
            "Epoch 10/20\n",
            "32/32 - 0s - loss: 0.0023 - 339ms/epoch - 11ms/step\n",
            "Epoch 11/20\n",
            "32/32 - 0s - loss: 0.0022 - 348ms/epoch - 11ms/step\n",
            "Epoch 12/20\n",
            "32/32 - 0s - loss: 0.0022 - 343ms/epoch - 11ms/step\n",
            "Epoch 13/20\n",
            "32/32 - 0s - loss: 0.0021 - 334ms/epoch - 10ms/step\n",
            "Epoch 14/20\n",
            "32/32 - 0s - loss: 0.0021 - 339ms/epoch - 11ms/step\n",
            "Epoch 15/20\n",
            "32/32 - 0s - loss: 0.0020 - 357ms/epoch - 11ms/step\n",
            "Epoch 16/20\n",
            "32/32 - 0s - loss: 0.0020 - 337ms/epoch - 11ms/step\n",
            "Epoch 17/20\n",
            "32/32 - 0s - loss: 0.0020 - 348ms/epoch - 11ms/step\n",
            "Epoch 18/20\n",
            "32/32 - 0s - loss: 0.0020 - 375ms/epoch - 12ms/step\n",
            "Epoch 19/20\n",
            "32/32 - 0s - loss: 0.0019 - 351ms/epoch - 11ms/step\n",
            "Epoch 20/20\n",
            "32/32 - 0s - loss: 0.0019 - 348ms/epoch - 11ms/step\n",
            "[RNN] 테스트 MSE: 0.00181\n",
            "\n",
            "Epoch 1/20\n",
            "32/32 - 2s - loss: 0.1091 - 2s/epoch - 64ms/step\n",
            "Epoch 2/20\n",
            "32/32 - 1s - loss: 0.0070 - 748ms/epoch - 23ms/step\n",
            "Epoch 3/20\n",
            "32/32 - 1s - loss: 0.0049 - 703ms/epoch - 22ms/step\n",
            "Epoch 4/20\n",
            "32/32 - 1s - loss: 0.0040 - 723ms/epoch - 23ms/step\n",
            "Epoch 5/20\n",
            "32/32 - 1s - loss: 0.0034 - 722ms/epoch - 23ms/step\n",
            "Epoch 6/20\n",
            "32/32 - 1s - loss: 0.0030 - 722ms/epoch - 23ms/step\n",
            "Epoch 7/20\n",
            "32/32 - 1s - loss: 0.0027 - 734ms/epoch - 23ms/step\n",
            "Epoch 8/20\n",
            "32/32 - 1s - loss: 0.0025 - 746ms/epoch - 23ms/step\n",
            "Epoch 9/20\n",
            "32/32 - 1s - loss: 0.0023 - 739ms/epoch - 23ms/step\n",
            "Epoch 10/20\n",
            "32/32 - 1s - loss: 0.0022 - 710ms/epoch - 22ms/step\n",
            "Epoch 11/20\n",
            "32/32 - 1s - loss: 0.0021 - 756ms/epoch - 24ms/step\n",
            "Epoch 12/20\n",
            "32/32 - 1s - loss: 0.0020 - 745ms/epoch - 23ms/step\n",
            "Epoch 13/20\n",
            "32/32 - 1s - loss: 0.0019 - 740ms/epoch - 23ms/step\n",
            "Epoch 14/20\n",
            "32/32 - 1s - loss: 0.0019 - 721ms/epoch - 23ms/step\n",
            "Epoch 15/20\n",
            "32/32 - 1s - loss: 0.0018 - 753ms/epoch - 24ms/step\n",
            "Epoch 16/20\n",
            "32/32 - 1s - loss: 0.0018 - 740ms/epoch - 23ms/step\n",
            "Epoch 17/20\n",
            "32/32 - 1s - loss: 0.0018 - 742ms/epoch - 23ms/step\n",
            "Epoch 18/20\n",
            "32/32 - 1s - loss: 0.0017 - 726ms/epoch - 23ms/step\n",
            "Epoch 19/20\n",
            "32/32 - 1s - loss: 0.0017 - 739ms/epoch - 23ms/step\n",
            "Epoch 20/20\n",
            "32/32 - 1s - loss: 0.0017 - 726ms/epoch - 23ms/step\n",
            "[Deep RNN] 테스트 MSE: 0.00165\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder-Decoder 구현\n",
        ": Encoder의 hidden state를 가져와서 Decoder의 초기 hidden state로 사용함"
      ],
      "metadata": {
        "id": "zsPJqtyX31XP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import layers, Sequential, Input\n",
        "\n",
        "class EncoderDecoder(Model):\n",
        "    def __init__(self, hidden_dim, encoder_input_shape, decoder_input_shape, num_classes):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        \n",
        "        # SimpleRNN으로 이루어진 Encoder\n",
        "        self.encoder = layers.SimpleRNN(hidden_dim, return_state=True,input_shape=encoder_input_shape)\n",
        "        # return_state: hidden_state를 쓰기 위함    \n",
        "\n",
        "        # SimpleRNN으로 이루어진 Decoder.\n",
        "        self.decoder = layers.SimpleRNN(hidden_dim, return_sequences=True, input_shape=decoder_input_shape)\n",
        "        \n",
        "        self.dense = layers.Dense(num_classes, activation=\"softmax\")\n",
        "        \n",
        "    def call(self, encoder_inputs, decoder_inputs):\n",
        "        # Encoder에 입력값을 넣어 Decoder의 초기 state로 사용할 state를 얻어냄\n",
        "        _, encoder_state = self.encoder(encoder_inputs) # _로 표현하여 값이 있지만 사용하지 않음을 명시적으로 나타냄\n",
        "        # - encoder_output은 필요없다. encoder의 hidden state만 필요!\n",
        "        \n",
        "        # Decoder에 입력값을 넣고, 초기 state는 Encoder에서 얻어낸 state로 설정\n",
        "        decoder_outputs = self.decoder(decoder_inputs, initial_state=[encoder_state])\n",
        "        \n",
        "        outputs = self.dense(decoder_outputs)\n",
        "        \n",
        "        return outputs\n",
        "\n",
        "\n",
        "def main():\n",
        "    # hidden state의 크기\n",
        "    hidden_dim = 20\n",
        "    \n",
        "    # Encoder에 들어갈 각 데이터의 모양\n",
        "    encoder_input_shape = (10, 1)\n",
        "    \n",
        "    # Decoder에 들어갈 각 데이터의 모양\n",
        "    decoder_input_shape = (30, 1)\n",
        "    \n",
        "    # 분류한 클래스 개수\n",
        "    num_classes = 5\n",
        "\n",
        "    # Encoder-Decoder 모델\n",
        "    model = EncoderDecoder(hidden_dim, encoder_input_shape, decoder_input_shape, num_classes)\n",
        "    \n",
        "    # 모델에 넣어줄 가상의 데이터를 생성\n",
        "    encoder_x, decoder_x = tf.random.uniform(shape=encoder_input_shape), tf.random.uniform(shape=decoder_input_shape)\n",
        "    encoder_x, decoder_x = tf.expand_dims(encoder_x, axis=0), tf.expand_dims(decoder_x, axis=0)\n",
        "    y = model(encoder_x, decoder_x)\n",
        "\n",
        "    # 모델의 정보를 출력\n",
        "    model.summary()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWkpiiHu1v3T",
        "outputId": "e6a341a7-b316-4a88-fdc6-e0d2c3556596"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder_decoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn_6 (SimpleRNN)    multiple                  440       \n",
            "                                                                 \n",
            " simple_rnn_7 (SimpleRNN)    multiple                  440       \n",
            "                                                                 \n",
            " dense_4 (Dense)             multiple                  105       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 985\n",
            "Trainable params: 985\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zDeyZCOQ4aWl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}