{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyesungKomet/rokaf_ai/blob/main/Elice_3_2_Tensorflow%EB%A1%9C_%EB%B0%B0%EC%9A%B0%EB%8A%94_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KSYf8ukEsAe"
      },
      "source": [
        "# 영상의 특징량\n",
        "\n",
        "### 고전적인 영상처리:\n",
        "이미지 전체가 학습데이터로 쓰이고\n",
        "1차원으로 풀어서 학습이 되다보니\n",
        "해상도가 바뀌거나 뒤집히는 등 영상이 바뀌면\n",
        "분류를 못해버린다\n",
        "![](https://srdas.github.io/DLBook/DL_images/mnistNN.png)\n",
        "여기서도 0이 geometric한 변화가 생기면 0으로 인식하지 못하게 됨(비대칭인 1같은 숫자는 더 그러겠지?)\n",
        "\n",
        "\n",
        "영상의 특징량: 영상을 수치적 값의 관점에서 추출한 정보  \n",
        "-->영상의 정보를 낮은 차원으로 처리할 수 있다!\n",
        "(입력으로 들어가는 데이터 양이 더 적어짐)\n",
        "\n",
        "* Scale invariant: Scale(가로 세로 비율)을 지키면서\n",
        "크기가 변해도 재학습 필요 없다\n",
        "\n",
        "* inter-class variation: 같은 인물의 다른 사진을 각각의\n",
        "클래스 단위로 분류함 - 고전에서도 됨\n",
        "* intra-class variation: 한 클래스 안에서의 변화(각도 등의 변화)\n",
        "를 구분해서 같은 클래스라고 인식\n",
        "* Photometric한 변화에도 분류 가능해짐!\n",
        "  * Noise corruption - 잡음(이상한 부분)찍힘\n",
        "  * Motion blurring - 움직여서 흐리게 찍힘\n",
        "  * Out-of-plane rotation - 영화관 등에서 크기 맞추려고 \n",
        "\t위아래에 그레이 스케일 줌\n",
        "  * Occlusion - 목표하는 물체가 다른 물체에 가려짐\n",
        "  * Illumination: 조도 낮게 줌\n",
        "  * Deformation: 왜곡(길어보이는 등)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rO5VsyjDGLa9"
      },
      "source": [
        "### 좋은 특징량의 조건\n",
        "\n",
        "* Repeatability  \n",
        "geometric(사진자체는 같은데 Scaling, Rotation, Cropping 등 발생),  \n",
        "photometric(사진 처리 s/w에서의 문제(위의 occlusion 등)\n",
        "이 둘의 변환에도 불구하고 똑같은 영상에서 동일한 특징이어야 한다\n",
        "\n",
        "* Saliency and Locality  \n",
        "Saliency: 내가 관심이 있는 영역만 보겠다!(배경 제거, 선따기)  \n",
        "영상의 interesting point를 포함해야 하며 영상 내의 작은 영역이어야 함(Locality)  \n",
        "ex) 영상 안의 한 물체만 작은 영역으로서 포함함\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sTXXJx1Go2Q"
      },
      "source": [
        "### 영상의 특징량의 종류 - Local VS Global\n",
        "* Global: 이미지가 통으로 feature가 되는 것 - 객체 감지, 분류 등  \n",
        "\tex) 윤곽선!\n",
        "* Local: 지역적으로만 관찰되는 feature  \n",
        "\tex) 내부 영역에 둥글고 빨간 영역 -> 사과!\n",
        "\n",
        "\n",
        "=> Local feature가 모여서 Global feature가 된다!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woYBHrUkG11V"
      },
      "source": [
        "# CNN\n",
        "\n",
        "### Convolution & Padding\n",
        "* Convolution: input으로 목적하는 output을 낼 때 주변 값들을 반영해서 filter 또는 mask 또는 Kernel를 3x3 등으로 해서 계산해냄  \n",
        "but Convolution만 하면 이미지가 점점 작아짐  \n",
        "  --> Padding!을 통해 크기 유지!  \n",
        "  --> 정보의 유실이 덜 일어나는 효과!  \n",
        "\n",
        "* Padding: 테두리에 0 등 큰 영향을 주지 않는 값을 채움  \n",
        "  --> 가장자리의 정보 유실을 방지할 수 있음\n",
        "\n",
        "* Feature Map: Convolution을 마친 결과  \n",
        "== 영상이 필터를 통과한 결과 행렬!!  \n",
        "\n",
        "![](https://miro.medium.com/max/1400/1*Kn3FMYOPMI6aQrzabC04gg.png)\n",
        "수평선 반영하는 커널과 수직 반영하는 커널 둘다 씌워주면\n",
        "윤곽선을 잘 딴 피처맵 얻을 수 있다!\n",
        "\n",
        "![](https://taewanmerepo.github.io/2018/01/cnn/conv2.jpg)\n",
        "컬러 영상의 경우 RGB 각 채널에 각각 알맞는 필터를 적용해서 \n",
        "얻는 Convolution의 Result 행렬을 합산해서 Feature Map을 구한다\n",
        "\n",
        "![](https://miro.medium.com/max/797/1*slzHFjXRkTVtFhRipPl8vw.png)\n",
        "커널을 하나만 사용하지 않고 64, 128개 등등 다양한 커널 사용  \n",
        "-> 다 적용시켜보면서 얻은 다양한 Feature Map을 합산해서\n",
        "분류가 가능하도록 함\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj3OjwS9Ixl9"
      },
      "source": [
        "### Stride & Pooling\n",
        "\n",
        "* Stride: Convolution에서 건너뛰기!  \n",
        "Convolution을 다 돌지 않고 스텝(보폭)을 크게 한다(default는 1)  \n",
        "DownSampling할 때 쓰임!  \n",
        "\n",
        "* Pooling: 강한 특징 가진 픽셀만 뽑겠다!(특징점만 강조하면서도 차원 낮추기 위해 사용)  \n",
        "Max Pooling, Average Pooling, Min Pooling 등등  \n",
        "스트라이드만으로는 정보의 유실이 발생할 수 있어서 이 유실을 최소화 하기 위해 풀링 사용  \n",
        "  -> 특징점 잘 뽑을 수 있다!\n",
        "\n",
        "Stride, Pooling 둘다 Down Sampling(표본화)하기 위해 사용  \n",
        "-> 정보의 수를 줄여 추론속도를 높일 수 있다!  \n",
        "\n",
        "* Stride) 단순히 다운 샘플링만 함\n",
        "* Pooling) 다운 샘플링을 하며 특징점을 강조하는 효과 있음!  \n",
        "\n",
        "\tbut Stride가 처리 속도는 더 빠름  \n",
        "\n",
        "=> 둘다  적절히 트레이드 오프하여 사용(속도와 이식률 측면에서)  \n",
        "\\+ Convolution의 끝단계쯤엔 이미 데이터가 작아서 잘 사용하지 않음\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDAFmvf2JkGs"
      },
      "source": [
        "### CNN(Convolution Neural Network)\n",
        "\n",
        "* 기존 영상 분류 시스템  \n",
        "Image -> Feature Extraction(검출기) -> Classification -> Classification Result  \n",
        "  * 검출기) SIFT, FURF, MOG, HOG, LOG, HAAR 등  \n",
        "  * 분류) KNN, Decision Tree, SVM, AdaBoost 등  \n",
        "![](https://mblogthumb-phinf.pstatic.net/20160330_250/dic1224_1459323262712kPNVH_JPEG/boosting.png?type=w2)\n",
        "weak classifer로 인간이면 눈, 코, 광대 등의 특징을 분류하고 이후 각 특징들에 가중치를 부여해서 합산한다. 이 때 예를 들어 3에 가까우면 Comet, 7에 가까우면 Hailey 뭐 이런식으로 누군지 분류한다는거지\n",
        "\n",
        "사람 인식하는 검출기와 차 인식하는 검출기 다름(필터가 달라야 함)  \n",
        "차의 경우 가로에 해당하는 필터가 더 많아야 분류가 용이하겠쥬?  \n",
        "-> 범용성이 떨어진다!! - 그때그떄 다른 검출기를 써야하니까...\n",
        "![](https://miro.medium.com/max/693/1*zUATaXMAmKof27rPyBRWsg.png)\n",
        "\n",
        "### CNN 구조\n",
        "\n",
        "Input -> (Convolution * Relu -> Pooling) 반복 -> Flatten -> Fully Connected -> Softmax  \n",
        "* Conv, Relu, Pooling은 Feature Learning이고  \n",
        "* Flatten, Fully Connected, Softmax는 분류에 해당하는 과정이다  \n",
        "![](https://miro.medium.com/max/1400/1*vkQ0hXDaQv57sALXAJquxA.jpeg)\n",
        "\n",
        "CNN은 Kernel의 계수를 학습하는거다!\n",
        "머신러닝에서는 Kernel을 직접 인간이 디자인해서 특징을 특정했다면  \n",
        "CNN은 데이터를 많이 돌려서 Kernel의 계수(Weight)를 학습하여 컴퓨터가 설정한다  \n",
        "-> 범용성 very good!  \n",
        "물체마다의 특징점을 잘 잡는 Kernel의 계수들이 그때그때 잘 뽑히겠지?\n",
        "\n",
        "일단 Kernel을 여러개 만들어서 다 통과시킨다  \n",
        "-> 많은 Feature나옴  \n",
        "분류계에서 결과로 가장 최적의 필터가 뭔지 알아냄"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jc35IUMXQ-gu"
      },
      "source": [
        "# FCN, 활성화 함수, 손실 함수\n",
        "### FCN\n",
        ": output layer쪽에서 결과값 출력을 위해 모든 노드를 연결함  \n",
        "각 결과값은 분류될 해당 클래스일 확률을 의미함  \n",
        "다 연결해야해서 연산량이 많음...\n",
        "FC Layer은 Dense Layer라고도 부름\n",
        "\n",
        "결과값이 구체적으로 어떤 확률을 의미하는지는 잘 모르겠... -> 활성화 함수(Softmax)\n",
        "\n",
        "* tanh, Sigmoid: 자극의 정도에 따라 On Off 구분  \n",
        "  이진분류 가능\n",
        "* Relu, softmax: 특정 세기 이상 자극이 오면 활성화됨  \n",
        "  미미할 땐 0 or 작은 값이다가 특정 구간부터 활성화됨\n",
        "\n",
        "SoftMax: CNN 분류에서 output layer 값이 일정하지 않을 수 있어서 이를 0~1 사이로 일반화! -> 각 클래스로 분류될 확률이 되며 전체 합은 1이 된다\n",
        "![](https://miro.medium.com/max/1400/1*gctBX5YHUUpBEK3MWD6r3Q.png)\n",
        "\n",
        "### 손실함수\n",
        "== 비용함수 == 목적함수\n",
        "cost function과 loss function은 다르긴 하다\n",
        "* 손실함수) 오차의 관점: 예측값과 정답의 차이가 얼마나 되는지\n",
        "* 비용함수) 최적화 관점: 모델이 얼마나 효용성이 있는지, 얼마나 최적값을 잘 찾게 해주는지\n",
        "* 목적함수) 목적의 관점: 얼마나 목적에 부합하는 결과를 잘 내주는지\n",
        "\n",
        "이거 말고도 loss function은 single data에 대한 예측과 정답 간의 차이를 의미하고 cost function은 전체 dataset에 대한 loss function, 즉 loss function의 평균 정도의 의미를 가지며 objective function은 최댓값, 최솟값을 구하는, 모델에서 훈련하여 최적화하는 모든 함수를 일컫는 말이다.  \n",
        "**즉 loss func <= cost func <= objective func**\n",
        "\n",
        "* MSE(Mean Square Error)\n",
        "![](https://blog.kakaocdn.net/dn/qJowI/btqBBUCMNDv/L7bfq2lu0hfsZzCSDx0E5k/img.png)\n",
        "  * |y - y^|는 컴퓨터와 수학 측면 모두에서 좋지 않음\n",
        "  절댓값은 if문으로 구간 나누어야 해서 좋지 않으며  \n",
        "  절댓값 함수는 미분불가능해서 수학 상으로도 좋지 않다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtJZrw5iD9Vb",
        "outputId": "7f7cd1fb-c108-49ed-9889-b21da4876f77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.04\n",
            "0.16\n",
            "951.4\n"
          ]
        }
      ],
      "source": [
        "# MSE\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def MSE(y, y_hat):\n",
        "    \"\"\"\n",
        "    MSE 함수 구현을 채우세요.\n",
        "    \"\"\"\n",
        "    error = y - y_hat\n",
        "    squared = error**2\n",
        "    mean = np.mean(squared)\n",
        "\n",
        "    # 소수점 둘째 자리에서 반올림하세요.\n",
        "    mean = round(mean, 2)\n",
        "    return mean\n",
        "    \n",
        "# 테스트1\n",
        "y     = np.array([0,   0,   1,   0,   0])\n",
        "y_hat = np.array([0.1, 0.1, 0.6, 0.1, 0.1])\n",
        "mse = MSE(y, y_hat)\n",
        "print(mse)\n",
        "\n",
        "# 테스트2\n",
        "y     = np.array([0,   0,   1,   0,   0])\n",
        "y_hat = np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n",
        "mse = MSE(y, y_hat)\n",
        "print(mse)\n",
        "\n",
        "# 테스트3\n",
        "y     = np.array([43,  7, 53, 86, -44])\n",
        "y_hat = np.array([54, 67, 23, 96, -50])\n",
        "mse = MSE(y, y_hat)\n",
        "print(mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnwnlTlIfPQL"
      },
      "source": [
        "# LeNet\n",
        "CNN의 첫번째 적용 모델\n",
        "\n",
        "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2021/03/Screenshot-from-2021-03-18-12-52-17.png)\n",
        "\n",
        "1. Convolution  \n",
        "5x5 kernel 6 channels stride = 1 \n",
        "  -> 입력값의 6가지 특징 파악  \n",
        "5x5 kernel은 3x3보다 많은(넓은) 특징 파악 가능\n",
        "\n",
        "2. Subsampling(== Downsampling)  \n",
        "Average Pooling by 2x2 kernel  \n",
        "  -> feature 크기를 1/4로 압축\n",
        "\n",
        "3. 1x1 Conv & FC layer  \n",
        "5x5x16을 5x5 커널로 120 채널 컨볼루션 \n",
        "  -> 1x1 120개 -> 1x120으로 Flattening  \n",
        "120개의 FC layer -> 84개의 FC layer  \n",
        "  -> 10가지 output layer -> 10가지 분류\n",
        "\n",
        "Conv, Pooling 모두 Actication은 tanh, Ouput은 softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwGj5coina06"
      },
      "source": [
        "## CNN 구현 예시\n",
        "![](https://www.bouvet.no/bouvet-deler/understanding-convolutional-neural-networks-part-2/_/image/3e0f39ab-6085-4ba4-8073-1eb0decf9f50:5463e218fce142bffa36c2563c7c15506842dfc3/width-768/CNN%20Layers.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgoAbMDffFDK",
        "outputId": "3e5746a1-aad4-4a57-b2d6-6cb67fbe13df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 3, 3, 64)          36928     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 576)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                36928     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 93,322\n",
            "Trainable params: 93,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# CNN 구현 예시\n",
        "from keras import datasets, layers, models, activations\n",
        "\n",
        "\n",
        "# 모델 변수를 선언합니다.\n",
        "#model = models.Sequential() # 순차적으로 layer를 쌓는 방식\n",
        "\n",
        "# 모델에 첫 번째 입력 레이어를 추가합니다. 첫 번째 layer에 한해서 input_shape을 지정해주면 유효하지 않은 입력 넣기 방지 가능\n",
        "#model.add(layers.Convolution2D(32, (3, 3), activation=activations.relu, input_shape=(28, 28, 1)))\n",
        "\n",
        "# Pooling\n",
        "#model.add(layers.MaxPooling2D()(2, 2)))\n",
        "\n",
        "# 1차원 텐서로 변환하기\n",
        "#model.add(layers.Flatten())\n",
        "\n",
        "# FC layer\n",
        "#model.add(layers.Dense(64, activation= 'relu'))\n",
        "\n",
        "# print model structure\n",
        "#model.summary()\n",
        "\n",
        "\n",
        "# 아래에 지시상항에 있는 모델 구조가 되도록 나머지 모델 구조를 선언해주세요.\n",
        "\"\"\"\n",
        "구현부\n",
        "\"\"\"\n",
        "model = models.Sequential()\n",
        "model.add(layers.Convolution2D(32, (3, 3), activation=activations.relu, input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Convolution2D(64, (3, 3), activation=activations.relu))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Convolution2D(64, (3, 3), activation=activations.relu))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation= 'relu'))\n",
        "model.add(layers.Dense(10, activation= 'softmax'))\n",
        "# Model 구조를 출력합니다.\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_YNQ8YBvoi0"
      },
      "source": [
        "## CNN 학습시키기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJI6R6cWm17M",
        "outputId": "cd5301e6-b01c-4475-f846-d5150453e8a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "4/4 [==============================] - 1s 24ms/step - loss: 2.2791 - categorical_accuracy: 0.0000e+00\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 2.1647 - categorical_accuracy: 0.1500\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 2.0233 - categorical_accuracy: 0.1500\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 1.8484 - categorical_accuracy: 0.0600\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 1.6066 - categorical_accuracy: 0.1000\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 1.3325 - categorical_accuracy: 0.1500\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 1.1266 - categorical_accuracy: 0.1600\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.7871 - categorical_accuracy: 0.1400\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.6985 - categorical_accuracy: 0.1400\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.5177 - categorical_accuracy: 0.1400\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.4616 - categorical_accuracy: 0.1300\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.4054 - categorical_accuracy: 0.1300\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.2840 - categorical_accuracy: 0.1200\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.2091 - categorical_accuracy: 0.1300\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.2048 - categorical_accuracy: 0.1300\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.1380 - categorical_accuracy: 0.1300\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.1248 - categorical_accuracy: 0.1400\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0800 - categorical_accuracy: 0.1300\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0964 - categorical_accuracy: 0.1300\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.0750 - categorical_accuracy: 0.1300\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_test_function.<locals>.test_function at 0x7f830d0b0d40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 - 0s - loss: 1.2336 - categorical_accuracy: 0.1500 - 137ms/epoch - 137ms/step\n",
            "test_loss: 1.2335660457611084 test_acc: 0.15000000596046448\n",
            "1/1 - 0s - 67ms/epoch - 67ms/step\n",
            "0 일 확률 = 0.04372046887874603\n",
            "1 일 확률 = 2.295988110745384e-08\n",
            "2 일 확률 = 0.010183890350162983\n",
            "3 일 확률 = 0.0001113087564590387\n",
            "4 일 확률 = 0.9437683820724487\n",
            "5 일 확률 = 0.00018519720470067114\n",
            "6 일 확률 = 1.0332465535611846e-05\n",
            "7 일 확률 = 0.0001831139816204086\n",
            "8 일 확률 = 2.3445684291800717e-06\n",
            "9 일 확률 = 0.0018348903395235538\n",
            "정답은 : [4]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy\n",
        "from tensorflow.keras import datasets, layers, models, activations, losses, optimizers, metrics\n",
        "\n",
        "# mnist 데이터 셋을 로드합니다.\n",
        "# 각각 학습셋(이미지, 라벨), 테스트 셋(이미지, 라벨)으로 구성이 되어 있습니다.\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
        "\n",
        "# 학습과 테스트에 사용할 데이터의 수를 조정합니다.\n",
        "# 참고, 원래 MNIST의 학습 이미지 셋은 60000개, 테스트 셋은 10000개 입니다.\n",
        "train_cnt, test_cnt = 100, 20\n",
        "train_images, train_labels = train_images[:train_cnt], train_labels[:train_cnt]\n",
        "test_images, test_labels = test_images[:test_cnt], test_labels[:test_cnt]\n",
        "\n",
        "# 학습 셋은 사용할 갯수만큼 28x28 이진 이미지이므로 reshaping을 해줍니다.\n",
        "train_images = train_images.reshape((train_cnt, 28, 28, 1))\n",
        "\n",
        "# 테스트 셋 역시 사용할 갯수만큼 28x28 이진 이미지이므로 reshaping을 해줍니다.\n",
        "test_images = test_images.reshape((test_cnt, 28, 28, 1))\n",
        "\n",
        "# 픽셀 값을 0~1 사이로 정규화합니다.\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "# 모델을 구조를 선언하세요.\n",
        "\"\"\"\n",
        "구현부\n",
        "\"\"\"\n",
        "model = models.Sequential()\n",
        "model.add(layers.Convolution2D(32, (3, 3), activation=activations.relu, input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Convolution2D(64, (3, 3), activation=activations.relu))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Convolution2D(64, (3, 3), activation=activations.relu))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation= 'relu'))\n",
        "model.add(layers.Dense(10, activation= 'softmax'))\n",
        "# 모델을 컴파일 합니다.\n",
        "adam_optimizer = optimizers.Adam()\n",
        "loss_function = losses.sparse_categorical_crossentropy\n",
        "metric = metrics.categorical_accuracy\n",
        "\n",
        "\"\"\"\n",
        "구현부\n",
        "\"\"\"\n",
        "model.compile(optimizer = adam_optimizer, loss=loss_function, metrics=[metric])\n",
        "\n",
        "# 모델을 학습데이터로 학습하세요.\n",
        "\"\"\"\n",
        "구현부\n",
        "\n",
        "\"\"\"\n",
        "model.fit(train_images, train_labels, epochs=20)\n",
        "\n",
        "# 모델을 평가하세요.\n",
        "\"\"\"\n",
        "test_loss, test_acc =\n",
        "\"\"\"\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
        "\n",
        "# 학습 결과를 출력합니다.\n",
        "print(\"test_loss:\", test_loss, \"test_acc:\", test_acc)\n",
        "\n",
        "# 모델에 테스트 이미지를 넣고 예측값을 확인해봅니다.\n",
        "test_img = cv2.imread(\"다운로드.png\", cv2.IMREAD_GRAYSCALE)\n",
        "#test_img = test_img[1]\n",
        "# 입력 이미지의 픽셀을 0~1 사이로 정규화 합니다.\n",
        "test_img = test_img / 255.0\n",
        "row, col, channel = test_img.shape[0], test_img.shape[1], 1\n",
        "confidence = model.predict(test_img.reshape((1, row, col, channel)), verbose=2)\n",
        "\n",
        "for i in range(confidence.shape[1]):\n",
        "    print(f\"{i} 일 확률 = {confidence[0][i]}\")\n",
        "\n",
        "print(f\"정답은 : {numpy.argmax(confidence, axis=1)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmQhSXrS4g5Y"
      },
      "source": [
        "## LeNet 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hGJsDWPwOV2",
        "outputId": "20ba6700-9aa5-4f47-c3b3-8084fd69f5ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_27 (Conv2D)          (None, 28, 28, 6)         156       \n",
            "                                                                 \n",
            " average_pooling2d (AverageP  (None, 14, 14, 6)        0         \n",
            " ooling2D)                                                       \n",
            "                                                                 \n",
            " conv2d_28 (Conv2D)          (None, 10, 10, 16)        2416      \n",
            "                                                                 \n",
            " average_pooling2d_1 (Averag  (None, 5, 5, 16)         0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " conv2d_29 (Conv2D)          (None, 1, 1, 120)         48120     \n",
            "                                                                 \n",
            " flatten_9 (Flatten)         (None, 120)               0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 84)                10164     \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 10)                850       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 61,706\n",
            "Trainable params: 61,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import logging, os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import tensorflow as tf\n",
        "logger = tf.get_logger()\n",
        "logger.setLevel(logging.FATAL)\n",
        "\n",
        "import keras\n",
        "from tensorflow.keras import datasets, layers, models, activations, utils\n",
        "\n",
        "# 모델 변수를 선언합니다.\n",
        "model = models.Sequential()\n",
        "# 모델에 첫번째 입력 레이어를 추가합니다.\n",
        "model.add(layers.Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation='tanh', input_shape=(32, 32, 1)))\n",
        "model.add(layers.AveragePooling2D((2,2), strides=(2,2)))\n",
        "model.add(layers.Conv2D(16, (5,5), strides=(1,1), activation='tanh'))\n",
        "model.add(layers.AveragePooling2D((2,2), strides=(2,2)))\n",
        "model.add(layers.Conv2D(120, (5,5), strides=(1,1)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(84, 'tanh'))\n",
        "model.add(layers.Dense(10, 'softmax'))\n",
        "\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy, optimizer='SGD')\n",
        "\n",
        "# Model 구조 확인\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_5Yikta6H_-"
      },
      "source": [
        "## LeNet 학습시키기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiQF8g4x4kVo",
        "outputId": "f61822ea-bbc0-4b8a-aaf4-c67735d4b191"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_images : (60000, 32, 32, 1) <class 'numpy.ndarray'>\n",
            "test_images : (10000, 32, 32, 1) <class 'numpy.ndarray'>\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2443 - categorical_accuracy: 0.0995\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1051 - categorical_accuracy: 0.0991\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0745 - categorical_accuracy: 0.0992\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0608 - categorical_accuracy: 0.0991\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0548 - categorical_accuracy: 0.0989\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0496 - categorical_accuracy: 0.0990\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0456 - categorical_accuracy: 0.0988\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0447 - categorical_accuracy: 0.0989\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0421 - categorical_accuracy: 0.0990\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0403 - categorical_accuracy: 0.0991\n",
            "313/313 - 1s - loss: 0.0425 - categorical_accuracy: 0.0986 - 684ms/epoch - 2ms/step\n",
            "test_loss: 0.042458441108465195 test_acc: 0.09860000014305115\n",
            "0 일 확률 = 0.005576697643846273\n",
            "1 일 확률 = 3.633176959283446e-08\n",
            "2 일 확률 = 0.0005590681103058159\n",
            "3 일 확률 = 0.00013972565648145974\n",
            "4 일 확률 = 9.087951184483245e-05\n",
            "5 일 확률 = 7.02238321537152e-05\n",
            "6 일 확률 = 0.00014394601748790592\n",
            "7 일 확률 = 1.9971910347749144e-08\n",
            "8 일 확률 = 0.9919529557228088\n",
            "9 일 확률 = 0.0014664139598608017\n",
            "정답은 : [8]\n"
          ]
        }
      ],
      "source": [
        "import os \n",
        "import cv2\n",
        "import numpy\n",
        "from tensorflow.keras import datasets, layers, models, activations, losses, optimizers, metrics\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# mnist 데이터 셋을 로드합니다.\n",
        "# 각각 학습셋(이미지, 라벨), 테스트 셋(이미지, 라벨)으로 구성이 되어 있습니다.\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
        "\n",
        "train_cnt, test_cnt = 60000, 10000\n",
        "train_images, train_labels = train_images[:train_cnt], train_labels[:train_cnt]\n",
        "test_images, test_labels = test_images[:test_cnt], test_labels[:test_cnt]\n",
        "\n",
        "# 학습 셋은 60000개의 28x28 이진 이미지이므로 reshaping을 해줍니다.\n",
        "train_images = train_images.reshape((train_cnt, 28, 28, 1))\n",
        "\n",
        "# 테스트 셋은 10000개의 28x28 이진 이미지이므로 reshaping을 해줍니다.\n",
        "test_images = test_images.reshape((test_cnt, 28, 28, 1))\n",
        "\n",
        "# LeNet의 입력은 32x32 이미지 입니다. 패딩을 주어서 28 x 28에서 32 x 32 이미지로 만듭니다.\n",
        "train_images = numpy.pad(train_images, [[0, 0], [2,2], [2,2], [0,0]], 'constant')\n",
        "test_images = numpy.pad(test_images, [[0, 0], [2,2], [2,2], [0,0]], 'constant')\n",
        "print('train_images :', train_images.shape, type(train_images))\n",
        "print('test_images :', test_images.shape, type(test_images))\n",
        "\n",
        "# 픽셀 값을 0~1 사이로 정규화합니다.\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "# 모델을 구조를 선언합니다.\n",
        "\"\"\"\n",
        "구현부\n",
        "\"\"\"\n",
        "# 모델 변수를 선언합니다.\n",
        "model = models.Sequential()\n",
        "# 모델에 첫번째 입력 레이어를 추가합니다.\n",
        "model.add(layers.Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation='tanh', input_shape=(32, 32, 1)))\n",
        "model.add(layers.AveragePooling2D((2,2), strides=(2,2)))\n",
        "model.add(layers.Conv2D(16, (5,5), strides=(1,1), activation='tanh'))\n",
        "model.add(layers.AveragePooling2D((2,2), strides=(2,2)))\n",
        "model.add(layers.Conv2D(120, (5,5), strides=(1,1)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(84, 'tanh'))\n",
        "model.add(layers.Dense(10, 'softmax'))\n",
        "\n",
        "# 모델을 컴파일 합니다.\n",
        "adam_optimizer = optimizers.Adam()\n",
        "loss_function = losses.sparse_categorical_crossentropy\n",
        "metric = metrics.categorical_accuracy\n",
        "\"\"\"\n",
        "구현부\n",
        "\"\"\"\n",
        "\n",
        "model.compile(loss=loss_function, optimizer= adam_optimizer, metrics = [metric])\n",
        "\n",
        "# 모델을 학습데이터로 학습합니다.\n",
        "\"\"\"\n",
        "구현부\n",
        "\n",
        "\"\"\"\n",
        "model.fit(train_images, train_labels, epochs=10)\n",
        "\n",
        "# 모델을 평가합니다.\n",
        "\"\"\"\n",
        "test_loss, test_acc =\n",
        "\"\"\"\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
        "# 학습 결과를 출력합니다.\n",
        "print(\"test_loss:\", test_loss, \"test_acc:\", test_acc)\n",
        "\n",
        "# 모델에 테스트 이미지를 넣고 예측값을 확인해봅니다.\n",
        "test_img = cv2.imread(\"다운로드.png\", cv2.IMREAD_GRAYSCALE)\n",
        "#print(test_img)\n",
        "\n",
        "# 입력 이미지의 픽셀을 0~1 사이로 정규화 합니다.\n",
        "test_img = test_img / 255.0\n",
        "row, col, channel = test_img.shape[0], test_img.shape[1], 1\n",
        "confidence = model.predict(test_img.reshape((1, row, col, channel)))\n",
        "\n",
        "for i in range(confidence.shape[1]):\n",
        "    print(f\"{i} 일 확률 = {confidence[0][i]}\")\n",
        "\n",
        "print(f\"정답은 : {numpy.argmax(confidence, axis=1)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkwpBAm36OM_"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Elice 3-2 Tensorflow로 배우는 CNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOlwqigx8cow4x/q2Ekg2bM",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}